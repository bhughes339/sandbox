{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"testing\").enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/17 13:57:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/11/17 13:57:39 WARN HiveMetaStore: Location: file:/Users/williamhughes/code/scratch_pad/spark-warehouse/test.db/sampletable specified for non-external table:sampletable\n",
      "+--------------------+-------------------+-------+\n",
      "|            col_name|          data_type|comment|\n",
      "+--------------------+-------------------+-------+\n",
      "|        _change_type|             string|   null|\n",
      "|     _commit_version|             bigint|   null|\n",
      "|   _commit_timestamp|          timestamp|   null|\n",
      "|         __Timestamp|             bigint|   null|\n",
      "|     __DeleteVersion|          timestamp|   null|\n",
      "|     __UpsertVersion|          timestamp|   null|\n",
      "|__DROP_EXPECTATIO...|map<string,boolean>|   null|\n",
      "|__MEETS_DROP_EXPE...|            boolean|   null|\n",
      "|__ALLOW_EXPECTATI...|map<string,boolean>|   null|\n",
      "+--------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS test\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS test.sampleTable\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE test.sampleTable (\n",
    "    _change_type string,\n",
    "    _commit_version bigint,\n",
    "    _commit_timestamp timestamp,\n",
    "    __Timestamp bigint,\n",
    "    __DeleteVersion timestamp,\n",
    "    __UpsertVersion timestamp,\n",
    "    __DROP_EXPECTATIONS_COL map<string,boolean>,\n",
    "    __MEETS_DROP_EXPECTATIONS boolean,\n",
    "    __ALLOW_EXPECTATIONS_COL map<string,boolean>\n",
    ")\n",
    "\"\"\")\n",
    "spark.sql(\"DESCRIBE TABLE test.sampleTable\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 'bigint'),\n",
       " ('_change_type', 'string'),\n",
       " ('_commit_version', 'bigint'),\n",
       " ('_commit_timestamp', 'timestamp'),\n",
       " ('__Timestamp', 'bigint'),\n",
       " ('__DeleteVersion', 'timestamp'),\n",
       " ('__UpsertVersion', 'timestamp'),\n",
       " ('__DROP_EXPECTATIONS_COL', 'map<string,boolean>'),\n",
       " ('__MEETS_DROP_EXPECTATIONS', 'boolean'),\n",
       " ('__ALLOW_EXPECTATIONS_COL', 'map<string,boolean>')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = spark.sql(\"DESCRIBE TABLE test.sampleTable\").collect()\n",
    "new_columns = {row.col_name: lit(None).cast(row.data_type) for row in rows if row.col_name.startswith(\"_\")}\n",
    "df = spark.createDataFrame([(2,)], schema=[\"test\"])\n",
    "df.withColumns(new_columns).dtypes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
